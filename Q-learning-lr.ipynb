{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% load_ext autoreload\n",
    "% autoreload 2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from model.cifar_utils import load_data\n",
    "import time\n",
    "import os \n",
    "from model.funcs import *\n",
    "from model.objective_function import *\n",
    "from model.DQN import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (50000, 3072)\n",
      "Training labels shape:  (50000,)\n",
      "Test data shape:  (10000, 3072)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "# data for objective net\n",
    "X_train, y_train, X_test, y_test = load_data()\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "X_train = X_train.astype(np.float32) - mean_image.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32) - mean_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create Objective Net and get necessary operations\n",
    "with tf.name_scope('obj_inputs'):\n",
    "    ob_xs = tf.placeholder(shape=[None,3072],dtype=tf.float32)\n",
    "    ob_ys = tf.placeholder(shape=[None,],dtype=tf.int64)\n",
    "    \n",
    "    \n",
    "train_feed_dict = {ob_xs:X_train,ob_ys:y_train}\n",
    "val_feed_dict = {ob_xs:X_test,ob_ys:y_test}\n",
    "\n",
    "obj_model = ObjectiveNets(input_dim=3072,hidden_dims=[100],num_classes=10)\n",
    "ob_ws = tf.placeholder(shape=[None,None],dtype=tf.float32) # used in assign ops\n",
    "ob_lr = tf.placeholder(shape=[],dtype=tf.float32)  # used in assign ops\n",
    "\n",
    "\n",
    "# initial learning rate\n",
    "lr_c = 1\n",
    "with tf.variable_scope('ObjNets',reuse=tf.AUTO_REUSE):\n",
    "    obj_lr = tf.get_variable(name='learning_rate',shape=[],initializer=tf.constant_initializer(lr_c,dtype=tf.float32))\n",
    "    lr_sum = tf.summary.scalar('learning_rate',obj_lr)\n",
    "\n",
    "# get loss,weight,gradient\n",
    "obj_ls = obj_model.loss(ob_xs,ob_ys)\n",
    "obj_w = obj_model.weights\n",
    "obj_g = obj_model.grads\n",
    "\n",
    "# get update and evaluate op\n",
    "obj_model.update(obj_lr)\n",
    "obj_up = obj_model.updates\n",
    "obj_eva,_ = obj_model.evaluate(ob_xs,ob_ys)\n",
    "eva_sum = tf.summary.scalar('val_acc',obj_eva)\n",
    "\n",
    "# get summary\n",
    "obj_sum = obj_model.summary\n",
    "obj_merge = tf.summary.merge([obj_sum,lr_sum])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create DQN\n",
    "with tf.name_scope('dqn_inputs'):\n",
    "    dqn_xs = tf.placeholder(shape=[None,6],dtype=tf.float32)\n",
    "    dqn_ys = tf.placeholder(shape=[None,2],dtype=tf.float32)\n",
    "    \n",
    "dqn = DQN(input_dim=6,hidden_dims=[32],num_classes=2)\n",
    "\n",
    "# get prediction,loss,update\n",
    "dqn_fw = dqn.forward(dqn_xs)\n",
    "dqn_ls = dqn.loss(dqn_xs,dqn_ys)\n",
    "#dqn.update(0.001)\n",
    "#dqn_up = dqn.updates\n",
    "\n",
    "# get summary\n",
    "dqn_sum = dqn.summary\n",
    "dqn_merge = tf.summary.merge(dqn_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# realize Q-learning with Experience Replay\n",
    "# initialization\n",
    "sess = tf.InteractiveSession()\n",
    "global_step = tf.Variable(0,name='global_step',trainable=False)\n",
    "\n",
    "# dqn update using adam gradient descent\n",
    "train_dqn = tf.train.AdamOptimizer(0.05).minimize(dqn_ls,global_step=global_step)\n",
    "\n",
    "log_dir = os.path.abspath('.') + '/log/'\n",
    "model_name = 'model_{}'.format(int(time.time()))\n",
    "writer = tf.summary.FileWriter(log_dir+model_name,sess.graph)\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# basic parameters\n",
    "episodes = 200\n",
    "T = 100\n",
    "epsilon = 0.2\n",
    "M = 3\n",
    "fg = FeatureGenerator(M)\n",
    "A = 1000\n",
    "EM = Experience_Memory(A)\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize x_\n",
    "x_init = sess.run(obj_w,feed_dict=train_feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0  epsilon:0.2\n",
      "episode 0 mean reward:0.05017929087122718\n",
      "episode 1  epsilon:0.185\n",
      "episode 1 mean reward:0.05181982501036267\n",
      "episode 2  epsilon:0.17\n",
      "episode 2 mean reward:0.05100051264967341\n",
      "episode 3  epsilon:0.155\n",
      "episode 3 mean reward:0.04998419615148851\n",
      "episode 4  epsilon:0.14\n",
      "episode 4 mean reward:0.051638724167718024\n",
      "episode 5  epsilon:0.125\n",
      "episode 5 mean reward:0.0491439898575212\n",
      "episode 6  epsilon:0.11\n",
      "episode 6 mean reward:0.0507019265703145\n",
      "episode 7  epsilon:0.09499999999999999\n",
      "episode 7 mean reward:0.0490632008242719\n",
      "episode 8  epsilon:0.07999999999999999\n",
      "episode 8 mean reward:0.04875857834289673\n",
      "episode 9  epsilon:0.065\n",
      "episode 9 mean reward:0.048842652701486154\n",
      "episode 10  epsilon:0.04999999999999999\n",
      "episode 10 mean reward:0.047952806863573415\n",
      "episode 11  epsilon:0.04999999999999999\n",
      "episode 11 mean reward:0.05039105895780759\n",
      "episode 12  epsilon:0.04999999999999999\n",
      "episode 12 mean reward:0.049349299740161834\n",
      "episode 13  epsilon:0.04999999999999999\n",
      "episode 13 mean reward:0.048459143709904416\n",
      "episode 14  epsilon:0.04999999999999999\n",
      "episode 14 mean reward:0.04870211182233252\n",
      "episode 15  epsilon:0.04999999999999999\n",
      "episode 15 mean reward:0.048127319521367735\n",
      "episode 16  epsilon:0.04999999999999999\n",
      "episode 16 mean reward:0.0481236259349346\n",
      "episode 17  epsilon:0.04999999999999999\n",
      "episode 17 mean reward:0.04831695080114974\n",
      "episode 18  epsilon:0.04999999999999999\n",
      "episode 18 mean reward:0.048145186672872334\n",
      "episode 19  epsilon:0.04999999999999999\n",
      "episode 19 mean reward:0.047781987792212224\n",
      "episode 20  epsilon:0.04999999999999999\n",
      "episode 20 mean reward:0.04795280686357342\n",
      "episode 21  epsilon:0.04999999999999999\n",
      "episode 21 mean reward:0.048296474455306534\n",
      "episode 22  epsilon:0.04999999999999999\n",
      "episode 22 mean reward:0.04845950831470581\n",
      "episode 23  epsilon:0.04999999999999999\n",
      "episode 23 mean reward:0.04829450262594392\n",
      "episode 24  epsilon:0.04999999999999999\n",
      "episode 24 mean reward:0.04795288088439946\n",
      "episode 25  epsilon:0.04999999999999999\n",
      "episode 25 mean reward:0.04829615448539003\n",
      "episode 26  epsilon:0.04999999999999999\n",
      "episode 26 mean reward:0.04829605978047754\n",
      "episode 27  epsilon:0.04999999999999999\n",
      "episode 27 mean reward:0.04831652003551765\n",
      "episode 28  epsilon:0.04999999999999999\n",
      "episode 28 mean reward:0.04854147536149945\n",
      "episode 29  epsilon:0.04999999999999999\n",
      "episode 29 mean reward:0.047952806863573415\n",
      "episode 30  epsilon:0.04999999999999999\n",
      "episode 30 mean reward:0.0483345594220229\n",
      "episode 31  epsilon:0.04999999999999999\n",
      "episode 31 mean reward:0.04812264075315085\n",
      "episode 32  epsilon:0.04999999999999999\n",
      "episode 32 mean reward:0.04829112724187816\n",
      "episode 33  epsilon:0.04999999999999999\n"
     ]
    }
   ],
   "source": [
    "rewards_sum = []\n",
    "for e in range(episodes):\n",
    "    reward_per_ep = []\n",
    "    with tf.variable_scope('ObjNets',reuse=tf.AUTO_REUSE):\n",
    "        for i in range(len(x_init)):\n",
    "            weight_name = 'weight_{}'.format(i)\n",
    "            w_old = tf.get_variable(weight_name)\n",
    "            sess.run(w_old.assign(ob_ws),feed_dict={ob_ws:x_init[weight_name]})\n",
    "            \n",
    "    # validate if reseting is successful\n",
    "    assert np.allclose(w_old.eval(),x_init['weight_{}'.format(i)])\n",
    "    \n",
    "    # reset learning rate to lr_c\n",
    "    sess.run(obj_lr.assign(lr_c))\n",
    "    assert np.allclose(sess.run(obj_lr),lr_c)\n",
    "    \n",
    "    # epsilon decay\n",
    "    if epsilon > 0.05:\n",
    "        epsilon = 0.2 - e*(0.2 - 0.05) / 10\n",
    "    print('episode {}  epsilon:{}'.format(e,epsilon))\n",
    "#     update for the first M times\n",
    "    for t in range(M):\n",
    "        ls_t,g_t,lr_t = sess.run([obj_ls,obj_g,obj_lr],feed_dict=train_feed_dict)  \n",
    "        fg.loss_memory_update(ls_t)\n",
    "        fg.ali_update(g_t)\n",
    "        # update x\n",
    "        sess.run(obj_up,feed_dict=train_feed_dict)\n",
    "        \n",
    "    # from M to T, train dqn\n",
    "    x_,dx_,ls_ = sess.run([obj_w,obj_g,obj_ls],feed_dict=train_feed_dict)\n",
    "    for t in range(T):\n",
    "        # generate state feature vector s_t\n",
    "        ls_t,g_t,lr_t,w_t = sess.run([obj_ls,obj_g,obj_lr,obj_w],feed_dict=train_feed_dict)\n",
    "        s_t = fg.generate_feature(t,lr_t,ls_t,g_t)\n",
    "        fg.loss_memory_update(ls_t)\n",
    "        fg.ali_update(g_t)\n",
    "        \n",
    "#         print('*'*10)\n",
    "#         print('episode {}/step {} objective_nets loss:{}'.format(e,t,ls_t))\n",
    "        \n",
    "        # time t dqn predict \n",
    "        forward_t = sess.run(dqn_fw,feed_dict={dqn_xs:s_t})\n",
    "        # time t action\n",
    "        action_t = e_greedy(forward_t,epsilon)\n",
    "        # action: 0 for half, 1 for keep\n",
    "        if action_t == 0:\n",
    "            lr_tn = 0.5*lr_t\n",
    "        else:\n",
    "            x_,dx_ = w_t,g_t\n",
    "            ls_ = ls_t\n",
    "            lr_tn = lr_c\n",
    "        sess.run(obj_lr.assign(ob_lr),feed_dict={ob_lr:lr_tn})\n",
    "#         print('episode {}/step {} current learning rate:{}'.format(e,t,obj_lr.eval()))\n",
    "        \n",
    "        # update w to next state\n",
    "        with tf.variable_scope('ObjNets',reuse=tf.AUTO_REUSE):\n",
    "            for i in range(len(x_)):\n",
    "                weight_name = 'weight_{}'.format(i)\n",
    "                grad_name = 'grad_{}'.format(i)\n",
    "                w_ = tf.get_variable(weight_name)\n",
    "                w_new = x_[weight_name] - lr_tn*dx_[grad_name]\n",
    "                sess.run(w_.assign(ob_ws),feed_dict={ob_ws:w_new})\n",
    "                \n",
    "        # time t+1 feature s_tn\n",
    "        ls_tn,g_tn,lr_tn = sess.run([obj_ls,obj_g,obj_lr],feed_dict=train_feed_dict)\n",
    "        s_tn = fg.generate_feature(t+1,lr_tn,ls_tn,g_tn)\n",
    "        fg.loss_memory_update(ls_tn)\n",
    "        fg.ali_update(g_tn)\n",
    "        \n",
    "        if action_t == 0:\n",
    "            reward_t = reward_function(ls_tn)\n",
    "        else:\n",
    "            reward_t = reward_function(ls_,c=0.12)\n",
    "#         print('episode {}/step {} current state reward:{}'.format(e,t,reward_t))\n",
    "        reward_per_ep.append(reward_t)\n",
    "        # add experience to memory\n",
    "        if t == T - 1:\n",
    "            forward_tn = None\n",
    "        else:\n",
    "            forward_tn = sess.run(dqn_fw,feed_dict={dqn_xs:s_tn})\n",
    "        labels_t = DQN_labels(forward_t,action_t,reward_t,forward_tn,gamma=0.99)\n",
    "        EM.add_experience(s_t,action_t,reward_t,s_tn,labels_t)\n",
    "#         print('episode {}/step {} Experience memory:{}'.format(e,t,EM.memory.qsize()))\n",
    "        \n",
    "        # get experience batch\n",
    "        experience_batch = EM.get_experience(batch_size)\n",
    "        # get inputs and labels for training DQN\n",
    "        states_trainDQN = get_csf_from_experience(experience_batch)\n",
    "        labels_trainDQN = get_labels_from_experience(experience_batch)\n",
    "        \n",
    "        # normalize state features\n",
    "        max_features = np.max(states_trainDQN,axis=0)\n",
    "        min_features = np.min(states_trainDQN,axis=0)\n",
    "        \n",
    "        normalized_features = 1 - 2*(states_trainDQN-min_features)/(max_features-min_features+1e-10)\n",
    "        \n",
    "        ls_dqn,_ = sess.run([dqn_ls,train_dqn],feed_dict={dqn_xs:normalized_features,dqn_ys:labels_trainDQN})\n",
    "#         print('DQN loss:{}'.format(ls_dqn))\n",
    "#         print('*'*10)\n",
    "        \n",
    "        obj_sum = sess.run(obj_merge,feed_dict=train_feed_dict)\n",
    "        dqn_sum = sess.run(dqn_merge,feed_dict={dqn_xs:normalized_features,dqn_ys:labels_trainDQN})\n",
    "\n",
    "        \n",
    "        if t % 50 == 0 or (t+1)==T:\n",
    "            obj_val,obj_eva_sum = sess.run([obj_eva,eva_sum],feed_dict=val_feed_dict)\n",
    "#             print('episode {}/step {} val_acc:{}'.format(e,t,obj_val))\n",
    "#             print('*'*10)\n",
    "            writer.add_summary(obj_eva_sum,e*T+t)\n",
    "        if t + 1 == T:\n",
    "            checkpoint_path = os.path.join(log_dir+model_name,'my_DQN.ckpt')\n",
    "            saver.save(sess,checkpoint_path,global_step=global_step)\n",
    "            \n",
    "        writer.add_summary(obj_sum,e*T+t)\n",
    "        writer.add_summary(dqn_sum,e*T+t)\n",
    "        \n",
    "    print('episode {} mean reward:{}'.format(e,np.mean(reward_per_ep)))\n",
    "    rewards_sum.append(np.mean(reward_per_ep))\n",
    "    \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
